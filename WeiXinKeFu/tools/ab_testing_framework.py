#!/usr/bin/env python3
"""
A/Bæµ‹è¯•æ¡†æ¶ - å¯¹æ¯”å‘é‡åŒ¹é…vs LLMå¢å¼ºåŒ¹é…
ç³»ç»ŸåŒ–è¯„ä¼°ä¸¤ç§æ–¹æ³•çš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import json
import sqlite3
import time
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.services.intent_matcher import IntentMatcher
from src.services.performance_monitor import init_performance_monitor, MatchingMetrics
from src.config.config import config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    method: str  # vector/hybrid_fast/hybrid_balanced/hybrid_accurate
    intent_id: int
    intent_name: str
    matches_found: int
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    response_time: float
    api_calls: int
    api_cost: float
    avg_score: float
    max_score: float
    avg_confidence: float

@dataclass
class ABTestReport:
    """A/Bæµ‹è¯•æŠ¥å‘Š"""
    test_id: str
    test_time: str
    total_intents: int
    total_profiles: int
    
    # æ–¹æ³•Aï¼ˆå‘é‡ï¼‰
    method_a_name: str
    method_a_results: List[TestResult]
    method_a_avg_precision: float
    method_a_avg_recall: float
    method_a_avg_f1: float
    method_a_avg_time: float
    method_a_total_cost: float
    
    # æ–¹æ³•Bï¼ˆLLMå¢å¼ºï¼‰
    method_b_name: str
    method_b_results: List[TestResult]
    method_b_avg_precision: float
    method_b_avg_recall: float
    method_b_avg_f1: float
    method_b_avg_time: float
    method_b_total_cost: float
    
    # æ¯”è¾ƒç»“æœ
    precision_improvement: float
    recall_improvement: float
    f1_improvement: float
    time_difference: float
    cost_difference: float
    statistical_significance: float
    winner: str
    recommendations: List[str]

class ABTestingFramework:
    """A/Bæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, db_path: str = "user_profiles.db"):
        self.db_path = db_path
        self.monitor = init_performance_monitor(db_path)
        self.ground_truth = {}  # å­˜å‚¨çœŸå®åŒ¹é…ï¼ˆäººå·¥æ ‡æ³¨ï¼‰
        
    def load_ground_truth(self, intent_id: int) -> List[int]:
        """
        åŠ è½½çœŸå®åŒ¹é…ç»“æœï¼ˆç†æƒ³æƒ…å†µä¸‹åº”è¯¥æœ‰äººå·¥æ ‡æ³¨ï¼‰
        è¿™é‡Œä½¿ç”¨è§„åˆ™ï¼š
        - åˆ›ä¸šåˆä¼™äºº(ID=16) -> æå››(ID=2)
        - Pythonå¼€å‘(ID=15) -> å¼ ä¸‰(ID=1), é’±ä¸ƒ(ID=5)
        - æŠ€æœ¯é¡¾é—®(ID=17) -> èµµå…­(ID=4)
        """
        ground_truth_map = {
            16: [2],        # åˆ›ä¸šåˆä¼™äºº -> æå››
            15: [1, 5],     # Pythonå¼€å‘ -> å¼ ä¸‰, é’±ä¸ƒ
            17: [4],        # æŠ€æœ¯é¡¾é—® -> èµµå…­
            10: [],         # æ‹›è˜äººæ‰ï¼ˆé€šç”¨ï¼‰
            11: [],         # å¯»æ‰¾å®¢æˆ·ï¼ˆé€šç”¨ï¼‰
            12: [1, 4],     # AIæŠ€æœ¯ä¸“å®¶ -> å¼ ä¸‰, èµµå…­
            13: [],         # å¯»æ‰¾å®¢æˆ·ï¼ˆé€šç”¨ï¼‰
            14: [1, 2]      # AIæœºå™¨äººæ–¹å‘åˆä¼™äºº -> å¼ ä¸‰, æå››
        }
        return ground_truth_map.get(intent_id, [])
    
    async def run_single_test(
        self,
        intent_id: int,
        intent_name: str,
        user_id: str,
        method: str,
        use_hybrid: bool = False,
        hybrid_mode: str = "balanced"
    ) -> TestResult:
        """è¿è¡Œå•æ¬¡æµ‹è¯•"""
        
        # åˆå§‹åŒ–åŒ¹é…å™¨
        matcher = IntentMatcher(
            db_path=self.db_path,
            use_ai=True,
            use_hybrid=use_hybrid,
            hybrid_mode=hybrid_mode
        )
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡ŒåŒ¹é…
        matches = await matcher.match_intent_with_profiles(
            intent_id=intent_id,
            user_id=user_id
        )
        
        # è®¡ç®—è€—æ—¶
        response_time = time.time() - start_time
        
        # è·å–çœŸå®åŒ¹é…
        true_matches = self.load_ground_truth(intent_id)
        
        # è·å–é¢„æµ‹çš„profile IDs
        predicted_ids = [m['profile_id'] for m in matches]
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        true_positives = len(set(predicted_ids) & set(true_matches))
        false_positives = len(set(predicted_ids) - set(true_matches))
        false_negatives = len(set(true_matches) - set(predicted_ids))
        
        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # è®¡ç®—å¹³å‡åˆ†æ•°å’Œç½®ä¿¡åº¦
        avg_score = np.mean([m['score'] for m in matches]) if matches else 0
        max_score = max([m['score'] for m in matches]) if matches else 0
        avg_confidence = np.mean([m.get('confidence', 0.5) for m in matches]) if matches else 0
        
        # ä¼°ç®—APIæˆæœ¬
        api_calls = len(matches) if use_hybrid else 0
        api_cost = api_calls * 0.01  # æ¯æ¬¡è°ƒç”¨0.01å…ƒ
        
        return TestResult(
            method=method,
            intent_id=intent_id,
            intent_name=intent_name,
            matches_found=len(matches),
            true_positives=true_positives,
            false_positives=false_positives,
            false_negatives=false_negatives,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            response_time=response_time,
            api_calls=api_calls,
            api_cost=api_cost,
            avg_score=avg_score,
            max_score=max_score,
            avg_confidence=avg_confidence
        )
    
    async def run_ab_test(
        self,
        user_id: str,
        intent_ids: Optional[List[int]] = None,
        method_a: str = "vector",
        method_b: str = "hybrid_accurate"
    ) -> ABTestReport:
        """è¿è¡Œå®Œæ•´çš„A/Bæµ‹è¯•"""
        
        print("\n" + "="*70)
        print("ğŸ§ª A/Bæµ‹è¯•å¼€å§‹")
        print("="*70)
        print(f"æ–¹æ³•A: {method_a}")
        print(f"æ–¹æ³•B: {method_b}")
        print()
        
        # è·å–è¦æµ‹è¯•çš„æ„å›¾
        if intent_ids is None:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, name FROM user_intents 
                WHERE user_id = ? AND status = 'active'
                ORDER BY priority DESC
                LIMIT 5
            """, (user_id,))
            intents = cursor.fetchall()
            conn.close()
        else:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            intents = []
            for intent_id in intent_ids:
                cursor.execute("SELECT id, name FROM user_intents WHERE id = ?", (intent_id,))
                intent = cursor.fetchone()
                if intent:
                    intents.append(intent)
            conn.close()
        
        # æ–¹æ³•Aæµ‹è¯•ç»“æœ
        method_a_results = []
        print(f"\nğŸ“Š æµ‹è¯•æ–¹æ³•A: {method_a}")
        print("-" * 50)
        
        for intent_id, intent_name in intents:
            print(f"æµ‹è¯•æ„å›¾: {intent_name} (ID={intent_id})")
            
            if method_a == "vector":
                result = await self.run_single_test(
                    intent_id, intent_name, user_id,
                    method="vector", use_hybrid=False
                )
            else:
                # æ··åˆæ¨¡å¼
                mode = method_a.replace("hybrid_", "")
                result = await self.run_single_test(
                    intent_id, intent_name, user_id,
                    method=method_a, use_hybrid=True, hybrid_mode=mode
                )
            
            method_a_results.append(result)
            print(f"  âœ… å®Œæˆ - æ‰¾åˆ°{result.matches_found}ä¸ªåŒ¹é…, "
                  f"ç²¾ç¡®ç‡:{result.precision:.2f}, å¬å›ç‡:{result.recall:.2f}, "
                  f"è€—æ—¶:{result.response_time:.2f}s")
        
        # æ–¹æ³•Bæµ‹è¯•ç»“æœ
        method_b_results = []
        print(f"\nğŸ“Š æµ‹è¯•æ–¹æ³•B: {method_b}")
        print("-" * 50)
        
        for intent_id, intent_name in intents:
            print(f"æµ‹è¯•æ„å›¾: {intent_name} (ID={intent_id})")
            
            if method_b == "vector":
                result = await self.run_single_test(
                    intent_id, intent_name, user_id,
                    method="vector", use_hybrid=False
                )
            else:
                # æ··åˆæ¨¡å¼
                mode = method_b.replace("hybrid_", "")
                result = await self.run_single_test(
                    intent_id, intent_name, user_id,
                    method=method_b, use_hybrid=True, hybrid_mode=mode
                )
            
            method_b_results.append(result)
            print(f"  âœ… å®Œæˆ - æ‰¾åˆ°{result.matches_found}ä¸ªåŒ¹é…, "
                  f"ç²¾ç¡®ç‡:{result.precision:.2f}, å¬å›ç‡:{result.recall:.2f}, "
                  f"è€—æ—¶:{result.response_time:.2f}s")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        method_a_avg_precision = np.mean([r.precision for r in method_a_results])
        method_a_avg_recall = np.mean([r.recall for r in method_a_results])
        method_a_avg_f1 = np.mean([r.f1_score for r in method_a_results])
        method_a_avg_time = np.mean([r.response_time for r in method_a_results])
        method_a_total_cost = sum([r.api_cost for r in method_a_results])
        
        method_b_avg_precision = np.mean([r.precision for r in method_b_results])
        method_b_avg_recall = np.mean([r.recall for r in method_b_results])
        method_b_avg_f1 = np.mean([r.f1_score for r in method_b_results])
        method_b_avg_time = np.mean([r.response_time for r in method_b_results])
        method_b_total_cost = sum([r.api_cost for r in method_b_results])
        
        # è®¡ç®—æ”¹è¿›ç‡
        precision_improvement = (method_b_avg_precision - method_a_avg_precision) / method_a_avg_precision * 100 if method_a_avg_precision > 0 else 0
        recall_improvement = (method_b_avg_recall - method_a_avg_recall) / method_a_avg_recall * 100 if method_a_avg_recall > 0 else 0
        f1_improvement = (method_b_avg_f1 - method_a_avg_f1) / method_a_avg_f1 * 100 if method_a_avg_f1 > 0 else 0
        time_difference = method_b_avg_time - method_a_avg_time
        cost_difference = method_b_total_cost - method_a_total_cost
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        statistical_significance = self.calculate_significance(method_a_results, method_b_results)
        
        # ç¡®å®šè·èƒœè€…
        if method_b_avg_f1 > method_a_avg_f1 and statistical_significance > 0.05:
            winner = method_b
        elif method_a_avg_f1 > method_b_avg_f1 and statistical_significance > 0.05:
            winner = method_a
        else:
            winner = "æ— æ˜¾è‘—å·®å¼‚"
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self.generate_recommendations(
            method_a, method_b, 
            precision_improvement, recall_improvement,
            time_difference, cost_difference
        )
        
        # åˆ›å»ºæŠ¥å‘Š
        report = ABTestReport(
            test_id=datetime.now().strftime("%Y%m%d_%H%M%S"),
            test_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            total_intents=len(intents),
            total_profiles=5,  # æµ‹è¯•æ•°æ®ä¸­æœ‰5ä¸ªè”ç³»äºº
            method_a_name=method_a,
            method_a_results=method_a_results,
            method_a_avg_precision=method_a_avg_precision,
            method_a_avg_recall=method_a_avg_recall,
            method_a_avg_f1=method_a_avg_f1,
            method_a_avg_time=method_a_avg_time,
            method_a_total_cost=method_a_total_cost,
            method_b_name=method_b,
            method_b_results=method_b_results,
            method_b_avg_precision=method_b_avg_precision,
            method_b_avg_recall=method_b_avg_recall,
            method_b_avg_f1=method_b_avg_f1,
            method_b_avg_time=method_b_avg_time,
            method_b_total_cost=method_b_total_cost,
            precision_improvement=precision_improvement,
            recall_improvement=recall_improvement,
            f1_improvement=f1_improvement,
            time_difference=time_difference,
            cost_difference=cost_difference,
            statistical_significance=statistical_significance,
            winner=winner,
            recommendations=recommendations
        )
        
        return report
    
    def calculate_significance(self, results_a: List[TestResult], results_b: List[TestResult]) -> float:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨F1åˆ†æ•°è¿›è¡Œæ¯”è¾ƒ
        f1_a = [r.f1_score for r in results_a]
        f1_b = [r.f1_score for r in results_b]
        
        # è®¡ç®—å‡å€¼å·®å¼‚
        mean_diff = abs(np.mean(f1_b) - np.mean(f1_a))
        
        # è®¡ç®—æ ‡å‡†è¯¯å·®
        se_a = np.std(f1_a) / np.sqrt(len(f1_a)) if len(f1_a) > 0 else 0
        se_b = np.std(f1_b) / np.sqrt(len(f1_b)) if len(f1_b) > 0 else 0
        se_combined = np.sqrt(se_a**2 + se_b**2)
        
        # è®¡ç®—zåˆ†æ•°
        z_score = mean_diff / se_combined if se_combined > 0 else 0
        
        # ç®€åŒ–çš„på€¼ä¼°ç®—
        if z_score > 2.58:
            return 0.01  # 99%ç½®ä¿¡åº¦
        elif z_score > 1.96:
            return 0.05  # 95%ç½®ä¿¡åº¦
        elif z_score > 1.65:
            return 0.10  # 90%ç½®ä¿¡åº¦
        else:
            return 0.50  # æ— æ˜¾è‘—å·®å¼‚
    
    def generate_recommendations(
        self,
        method_a: str,
        method_b: str,
        precision_imp: float,
        recall_imp: float,
        time_diff: float,
        cost_diff: float
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºç²¾ç¡®ç‡å’Œå¬å›ç‡
        if recall_imp > 50:
            recommendations.append(f"ğŸ¯ {method_b}æ˜¾è‘—æå‡å¬å›ç‡({recall_imp:.1f}%)ï¼Œå»ºè®®åœ¨éœ€è¦å…¨é¢è¦†ç›–çš„åœºæ™¯ä½¿ç”¨")
        elif recall_imp < -50:
            recommendations.append(f"âš ï¸ {method_b}å¬å›ç‡ä¸‹é™({recall_imp:.1f}%)ï¼Œéœ€è¦ä¼˜åŒ–åŒ¹é…é˜ˆå€¼")
        
        if precision_imp > 30:
            recommendations.append(f"âœ… {method_b}ç²¾ç¡®ç‡æå‡({precision_imp:.1f}%)ï¼Œå‡å°‘äº†è¯¯åŒ¹é…")
        elif precision_imp < -30:
            recommendations.append(f"âš ï¸ {method_b}ç²¾ç¡®ç‡ä¸‹é™({precision_imp:.1f}%)ï¼Œå¯èƒ½äº§ç”Ÿè¿‡å¤šè¯¯åŒ¹é…")
        
        # åŸºäºæ—¶é—´å’Œæˆæœ¬
        if time_diff > 10:
            recommendations.append(f"â±ï¸ {method_b}å“åº”æ—¶é—´å¢åŠ {time_diff:.1f}ç§’ï¼Œè€ƒè™‘ä½¿ç”¨ç¼“å­˜æˆ–æ‰¹å¤„ç†ä¼˜åŒ–")
        elif time_diff < -5:
            recommendations.append(f"âš¡ {method_b}å“åº”é€Ÿåº¦æå‡{abs(time_diff):.1f}ç§’")
        
        if cost_diff > 0.5:
            recommendations.append(f"ğŸ’° {method_b}æˆæœ¬å¢åŠ Â¥{cost_diff:.2f}ï¼Œå»ºè®®åœ¨é«˜ä»·å€¼åœºæ™¯ä½¿ç”¨")
        
        # ç»¼åˆå»ºè®®
        if recall_imp > 30 and time_diff < 20:
            recommendations.append("ğŸ† å»ºè®®é‡‡ç”¨LLMå¢å¼ºåŒ¹é…ä½œä¸ºä¸»è¦æ–¹æ³•ï¼Œæ˜¾è‘—æå‡åŒ¹é…è´¨é‡")
        elif recall_imp < 0 and time_diff > 10:
            recommendations.append("ğŸ’¡ å»ºè®®ä¼˜åŒ–LLMæç¤ºè¯æˆ–ä½¿ç”¨æ›´å¿«çš„åŒ¹é…æ¨¡å¼")
        
        if not recommendations:
            recommendations.append("ğŸ“Š ä¸¤ç§æ–¹æ³•è¡¨ç°ç›¸è¿‘ï¼Œå¯æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©")
        
        return recommendations
    
    def print_report(self, report: ABTestReport):
        """æ‰“å°A/Bæµ‹è¯•æŠ¥å‘Š"""
        
        print("\n" + "="*70)
        print("ğŸ“Š A/Bæµ‹è¯•æŠ¥å‘Š")
        print("="*70)
        print(f"æµ‹è¯•ID: {report.test_id}")
        print(f"æµ‹è¯•æ—¶é—´: {report.test_time}")
        print(f"æµ‹è¯•è§„æ¨¡: {report.total_intents}ä¸ªæ„å›¾, {report.total_profiles}ä¸ªè”ç³»äºº")
        
        print("\n" + "-"*70)
        print(f"æ–¹æ³•A: {report.method_a_name}")
        print("-"*70)
        print(f"å¹³å‡ç²¾ç¡®ç‡: {report.method_a_avg_precision:.2%}")
        print(f"å¹³å‡å¬å›ç‡: {report.method_a_avg_recall:.2%}")
        print(f"å¹³å‡F1åˆ†æ•°: {report.method_a_avg_f1:.2%}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {report.method_a_avg_time:.2f}ç§’")
        print(f"æ€»APIæˆæœ¬: Â¥{report.method_a_total_cost:.2f}")
        
        print("\n" + "-"*70)
        print(f"æ–¹æ³•B: {report.method_b_name}")
        print("-"*70)
        print(f"å¹³å‡ç²¾ç¡®ç‡: {report.method_b_avg_precision:.2%}")
        print(f"å¹³å‡å¬å›ç‡: {report.method_b_avg_recall:.2%}")
        print(f"å¹³å‡F1åˆ†æ•°: {report.method_b_avg_f1:.2%}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {report.method_b_avg_time:.2f}ç§’")
        print(f"æ€»APIæˆæœ¬: Â¥{report.method_b_total_cost:.2f}")
        
        print("\n" + "-"*70)
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”")
        print("-"*70)
        print(f"ç²¾ç¡®ç‡å˜åŒ–: {report.precision_improvement:+.1f}%")
        print(f"å¬å›ç‡å˜åŒ–: {report.recall_improvement:+.1f}%")
        print(f"F1åˆ†æ•°å˜åŒ–: {report.f1_improvement:+.1f}%")
        print(f"å“åº”æ—¶é—´å·®å¼‚: {report.time_difference:+.2f}ç§’")
        print(f"æˆæœ¬å·®å¼‚: Â¥{report.cost_difference:+.2f}")
        print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p={report.statistical_significance:.3f}")
        
        print("\n" + "-"*70)
        print(f"ğŸ† è·èƒœè€…: {report.winner}")
        print("-"*70)
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(report.recommendations, 1):
            print(f"{i}. {rec}")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self.save_report(report)
    
    def save_report(self, report: ABTestReport):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        filename = f"ab_test_report_{report.test_id}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_dict = asdict(report)
        
        # å°†TestResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        report_dict['method_a_results'] = [asdict(r) for r in report.method_a_results]
        report_dict['method_b_results'] = [asdict(r) for r in report.method_b_results]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ§ª LLMå¢å¼ºæ„å›¾åŒ¹é… A/Bæµ‹è¯•æ¡†æ¶")
    print("="*70)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"API Key: {'âœ… å·²é…ç½®' if config.qwen_api_key else 'âŒ æœªé…ç½®'}")
    
    # åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
    ab_tester = ABTestingFramework()
    
    # çœŸå®ç”¨æˆ·ID
    user_id = "wm0gZOdQAAv-phiLJWS77wmzQQSOrL1Q"
    
    # è¿è¡ŒA/Bæµ‹è¯•
    print("\nå¼€å§‹A/Bæµ‹è¯•...")
    
    # æµ‹è¯•1: å‘é‡ vs LLMç²¾ç¡®æ¨¡å¼
    print("\n" + "="*70)
    print("æµ‹è¯•1: ä¼ ç»Ÿå‘é‡ vs LLMæ··åˆç²¾ç¡®æ¨¡å¼")
    print("="*70)
    
    report1 = await ab_tester.run_ab_test(
        user_id=user_id,
        intent_ids=[16, 15, 17],  # åˆ›ä¸šåˆä¼™äººã€Pythonå¼€å‘ã€æŠ€æœ¯é¡¾é—®
        method_a="vector",
        method_b="hybrid_accurate"
    )
    
    ab_tester.print_report(report1)
    
    # æµ‹è¯•2: å¿«é€Ÿæ¨¡å¼ vs ç²¾ç¡®æ¨¡å¼
    print("\n" + "="*70)
    print("æµ‹è¯•2: æ··åˆå¿«é€Ÿæ¨¡å¼ vs æ··åˆç²¾ç¡®æ¨¡å¼")
    print("="*70)
    
    report2 = await ab_tester.run_ab_test(
        user_id=user_id,
        intent_ids=[16, 15, 17],
        method_a="hybrid_fast",
        method_b="hybrid_accurate"
    )
    
    ab_tester.print_report(report2)
    
    print("\n" + "="*70)
    print("âœ… A/Bæµ‹è¯•å®Œæˆï¼")
    print("="*70)

if __name__ == "__main__":
    asyncio.run(main())