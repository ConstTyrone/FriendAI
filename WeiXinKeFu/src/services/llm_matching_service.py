"""
LLMæ„å›¾åŒ¹é…åˆ¤æ–­æœåŠ¡
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰ç†è§£å’Œç²¾ç¡®åŒ¹é…åˆ¤æ–­
æ”¯æŒæ•°æ®æ”¶é›†ã€è‡ªé€‚åº”æ ¡å‡†å’ŒA/Bæµ‹è¯•
"""

import json
import asyncio
import hashlib
import logging
import requests
import random
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class MatchStrategy(Enum):
    """åŒ¹é…ç­–ç•¥æšä¸¾"""
    VECTOR_ONLY = "vector_only"  # ä»…å‘é‡
    LLM_ONLY = "llm_only"  # ä»…LLM
    HYBRID = "hybrid"  # æ··åˆ
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”

class ComplexityLevel(Enum):
    """æ„å›¾å¤æ‚åº¦çº§åˆ«"""
    SIMPLE = "simple"  # ç®€å•
    MODERATE = "moderate"  # ä¸­ç­‰
    COMPLEX = "complex"  # å¤æ‚

@dataclass
class MatchJudgment:
    """LLMåŒ¹é…åˆ¤æ–­ç»“æœ"""
    match_score: float  # åŒ¹é…åˆ†æ•° 0-1
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    is_match: bool  # æ˜¯å¦åŒ¹é…
    matched_aspects: List[str]  # åŒ¹é…çš„æ–¹é¢
    missing_aspects: List[str]  # ç¼ºå¤±çš„æ–¹é¢
    explanation: str  # è¯¦ç»†è§£é‡Š
    suggestions: Optional[str] = None  # æ”¹è¿›å»ºè®®
    strategy_used: str = "llm"  # ä½¿ç”¨çš„ç­–ç•¥
    processing_time: float = 0.0  # å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
    cached: bool = False  # æ˜¯å¦æ¥è‡ªç¼“å­˜

class LLMMatchingService:
    """LLMæ„å›¾åŒ¹é…æœåŠ¡"""
    
    def __init__(self, qwen_api_key: str, db_path: str = "user_profiles.db", api_endpoint: str = None, user_id: str = None):
        """
        åˆå§‹åŒ–LLMåŒ¹é…æœåŠ¡
        
        Args:
            qwen_api_key: é€šä¹‰åƒé—®APIå¯†é’¥
            db_path: æ•°æ®åº“è·¯å¾„
            api_endpoint: APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºæ•°æ®æ”¶é›†ï¼‰
        """
        self.api_key = qwen_api_key
        self.db_path = db_path
        self.api_endpoint = api_endpoint or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.user_id = user_id
        self.cache = {}  # å†…å­˜ç¼“å­˜
        self.cache_ttl = timedelta(hours=24)  # ç¼“å­˜æœ‰æ•ˆæœŸ
        
        # é…ç½®å‚æ•°
        self.batch_size = 5  # æ‰¹å¤„ç†å¤§å°
        self.max_parallel = 3  # æœ€å¤§å¹¶è¡Œæ•°
        self.timeout = 30  # APIè°ƒç”¨è¶…æ—¶ï¼ˆç§’ï¼‰
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        # åˆå§‹åŒ–é…ç½®å’Œåˆ†ææœåŠ¡
        try:
            from ..config.scoring_config import scoring_config_manager
            self.config_manager = scoring_config_manager
            self.config = self.config_manager.get_config()
            logger.info(f"âœ… åŠ è½½è¯„åˆ†é…ç½®: ç­–ç•¥={self.config.strategy}")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½é…ç½®ç®¡ç†å™¨: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.config_manager = None
            self.config = None
        
        # åˆå§‹åŒ–æ•°æ®åˆ†ææœåŠ¡
        try:
            from .scoring_analytics import scoring_analytics
            self.analytics = scoring_analytics if self.config and self.config.enable_analytics else None
            logger.info(f"âœ… æ•°æ®åˆ†ææœåŠ¡: {'å·²å¯ç”¨' if self.analytics else 'å·²ç¦ç”¨'}")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½æ•°æ®åˆ†ææœåŠ¡: {e}")
            self.analytics = None
        
        # æ ¡å‡†å‚æ•°ç¼“å­˜
        self.calibration_params = None
        self.calibration_update_time = None
        
        logger.info(f"âœ… LLMåŒ¹é…æœåŠ¡åˆå§‹åŒ–æˆåŠŸ (æ•°æ®é£è½®æ¨¡å¼)")
        logger.info(f"   APIç«¯ç‚¹: {self.api_endpoint}")
    
    async def judge_match(
        self,
        intent: Dict,
        profile: Dict,
        context: Optional[Dict] = None,
        use_cache: bool = True
    ) -> MatchJudgment:
        """
        ä½¿ç”¨LLMåˆ¤æ–­æ„å›¾ä¸è”ç³»äººæ˜¯å¦åŒ¹é…ï¼ˆæ”¯æŒæ•°æ®é£è½®ï¼‰
        
        Args:
            intent: æ„å›¾ä¿¡æ¯
            profile: è”ç³»äººä¿¡æ¯
            context: é¢å¤–ä¸Šä¸‹æ–‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            MatchJudgment: åŒ¹é…åˆ¤æ–­ç»“æœ
        """
        start_time = datetime.now()
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cache_key = self._generate_cache_key(intent, profile)
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                cached_result.cached = True
                logger.info(f"ğŸ¯ å‘½ä¸­ç¼“å­˜: {cache_key[:16]}...")
                return cached_result
        
        try:
            # è·å–å½“å‰ç­–ç•¥ï¼ˆæ”¯æŒA/Bæµ‹è¯•ï¼‰
            strategy = self._get_current_strategy()
            
            # æ„å»ºprompt
            prompt = self._build_judgment_prompt(intent, profile, context, strategy)
            
            # è°ƒç”¨LLM
            response = await self._call_llm(prompt)
            
            # è®°å½•LLMåŸå§‹å“åº”
            logger.info(f"ğŸ¤– LLMåŸå§‹å“åº” (ç­–ç•¥={strategy}):\n{response}")
            
            # è§£æå“åº”
            judgment = self._parse_judgment(response)
            
            # åº”ç”¨è‡ªé€‚åº”æ ¡å‡†
            if self.config and self.config.enable_calibration:
                original_score = judgment.match_score
                judgment = await self._apply_calibration(judgment, intent, profile)
                if judgment.match_score != original_score:
                    logger.info(f"ğŸ”§ æ ¡å‡†è°ƒæ•´: {original_score:.3f} â†’ {judgment.match_score:.3f}")
            
            # è®°å½•è§£æç»“æœ
            logger.info(f"ğŸ“Š LLMæœ€ç»ˆç»“æœ: åˆ†æ•°={judgment.match_score:.3f}, ç½®ä¿¡åº¦={judgment.confidence:.3f}, æ˜¯å¦åŒ¹é…={judgment.is_match}")
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            judgment.processing_time = (datetime.now() - start_time).total_seconds()
            judgment.strategy_used = strategy
            
            # è®°å½•åˆ°æ•°æ®åˆ†ææœåŠ¡
            if self.analytics and self.user_id:
                try:
                    self.analytics.record_scoring(
                        user_id=self.user_id,
                        intent=intent,
                        profile=profile,
                        llm_score=judgment.match_score,
                        final_score=judgment.match_score,
                        confidence=judgment.confidence,
                        matched_aspects=judgment.matched_aspects,
                        missing_aspects=judgment.missing_aspects,
                        explanation=judgment.explanation,
                        strategy=strategy,
                        processing_time=judgment.processing_time
                    )
                except Exception as e:
                    logger.warning(f"è®°å½•è¯„åˆ†æ•°æ®å¤±è´¥: {e}")
            
            # å­˜å…¥ç¼“å­˜
            if use_cache:
                self._save_to_cache(cache_key, judgment)
            
            logger.info(f"âœ… LLMåˆ¤æ–­å®Œæˆ: åˆ†æ•°={judgment.match_score:.2f}, è€—æ—¶={judgment.processing_time:.2f}s")
            return judgment
            
        except Exception as e:
            logger.error(f"âŒ LLMåˆ¤æ–­å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return MatchJudgment(
                match_score=0.0,
                confidence=0.0,
                is_match=False,
                matched_aspects=[],
                missing_aspects=[],
                explanation=f"LLMåˆ¤æ–­å¤±è´¥: {str(e)}",
                processing_time=(datetime.now() - start_time).total_seconds()
            )
    
    async def batch_judge(
        self,
        intent: Dict,
        profiles: List[Dict],
        strategy: MatchStrategy = MatchStrategy.ADAPTIVE,
        parallel: bool = True
    ) -> List[MatchJudgment]:
        """
        æ‰¹é‡åˆ¤æ–­å¤šä¸ªè”ç³»äºº
        
        Args:
            intent: æ„å›¾ä¿¡æ¯
            profiles: è”ç³»äººåˆ—è¡¨
            strategy: åŒ¹é…ç­–ç•¥
            parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
            
        Returns:
            åˆ¤æ–­ç»“æœåˆ—è¡¨
        """
        if not profiles:
            return []
        
        # æ ¹æ®ç­–ç•¥è¿‡æ»¤å€™é€‰
        candidates = await self._filter_candidates(intent, profiles, strategy)
        
        if parallel and len(candidates) > 1:
            # å¹¶è¡Œå¤„ç†
            return await self._parallel_judge(intent, candidates)
        else:
            # ä¸²è¡Œå¤„ç†
            results = []
            for profile in candidates:
                judgment = await self.judge_match(intent, profile)
                results.append(judgment)
            return results
    
    async def explain_mismatch(
        self,
        intent: Dict,
        profile: Dict
    ) -> str:
        """
        è§£é‡Šä¸ºä»€ä¹ˆä¸åŒ¹é…
        
        Args:
            intent: æ„å›¾ä¿¡æ¯
            profile: è”ç³»äººä¿¡æ¯
            
        Returns:
            ä¸åŒ¹é…çš„è¯¦ç»†è§£é‡Š
        """
        prompt = self._build_mismatch_prompt(intent, profile)
        
        try:
            response = await self._call_llm(prompt)
            return response
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¸åŒ¹é…è§£é‡Šå¤±è´¥: {e}")
            return "æ— æ³•ç”Ÿæˆè¯¦ç»†è§£é‡Š"
    
    def assess_complexity(self, intent: Dict) -> ComplexityLevel:
        """
        è¯„ä¼°æ„å›¾å¤æ‚åº¦
        
        Args:
            intent: æ„å›¾ä¿¡æ¯
            
        Returns:
            å¤æ‚åº¦çº§åˆ«
        """
        score = 0
        
        # æ£€æŸ¥æ¡ä»¶æ•°é‡
        conditions = intent.get('conditions', {})
        required = conditions.get('required', [])
        preferred = conditions.get('preferred', [])
        
        score += len(required) * 2
        score += len(preferred)
        
        # æ£€æŸ¥æè¿°é•¿åº¦å’Œå¤æ‚åº¦
        description = intent.get('description', '')
        if len(description) > 200:
            score += 3
        if any(word in description for word in ['ä¸è¦', 'é™¤äº†', 'ä½†æ˜¯', 'æˆ–è€…']):
            score += 2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤æ‚é€»è¾‘
        if 'ä¸”' in description or 'æˆ–' in description:
            score += 2
        
        # åˆ¤æ–­å¤æ‚åº¦çº§åˆ«
        if score <= 3:
            return ComplexityLevel.SIMPLE
        elif score <= 8:
            return ComplexityLevel.MODERATE
        else:
            return ComplexityLevel.COMPLEX
    
    def _build_judgment_prompt(
        self,
        intent: Dict,
        profile: Dict,
        context: Optional[Dict] = None,
        strategy: Optional[str] = None
    ) -> str:
        """æ„å»ºåˆ¤æ–­prompt - æ”¯æŒå¤šç­–ç•¥"""
        
        # æå–æ„å›¾ä¿¡æ¯
        intent_desc = intent.get('description', 'æ— æè¿°')
        intent_name = intent.get('name', 'æœªå‘½å')
        
        # æå–æ¡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        conditions_text = ""
        conditions = intent.get('conditions', {})
        if conditions:
            required = conditions.get('required', [])
            preferred = conditions.get('preferred', [])
            
            if required:
                conditions_text += "\nå¿…è¦æ¡ä»¶ï¼š"
                for req in required:
                    if isinstance(req, dict):
                        field = req.get('field', '')
                        value = req.get('value', '')
                        conditions_text += f"\n- {field}: {value}"
                    else:
                        conditions_text += f"\n- {req}"
            
            if preferred:
                conditions_text += "\nåå¥½æ¡ä»¶ï¼š"
                for pref in preferred:
                    if isinstance(pref, dict):
                        field = pref.get('field', '')
                        value = pref.get('value', '')
                        conditions_text += f"\n- {field}: {value}"
                    else:
                        conditions_text += f"\n- {pref}"
        
        # æå–è”ç³»äººä¿¡æ¯
        profile_text = f"å§“åï¼š{profile.get('profile_name', profile.get('name', 'æœªçŸ¥'))}"
        
        # åŸºæœ¬ä¿¡æ¯
        basic_info = profile.get('basic_info', {})
        if basic_info:
            for key, value in basic_info.items():
                if value:
                    # å°†å­—æ®µåè½¬æ¢ä¸ºä¸­æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    field_name_map = {
                        'gender': 'æ€§åˆ«',
                        'age': 'å¹´é¾„',
                        'location': 'æ‰€åœ¨åœ°',
                        'education': 'å­¦å†/å­¦æ ¡',
                        'company': 'å…¬å¸',
                        'position': 'èŒä½',
                        'marital_status': 'å©šè‚²',
                        'asset_level': 'èµ„äº§æ°´å¹³',
                        'personality': 'æ€§æ ¼'
                    }
                    field_name = field_name_map.get(key, key)
                    profile_text += f"\n{field_name}ï¼š{value}"
        
        # æ ‡ç­¾
        tags = profile.get('tags', [])
        if tags:
            if isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except:
                    pass
            if isinstance(tags, list) and tags:
                profile_text += f"\næ ‡ç­¾ï¼š{', '.join(str(t) for t in tags)}"
        
        # AIæ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        if profile.get('ai_summary'):
            profile_text += f"\nAIæ‘˜è¦ï¼š{profile['ai_summary']}"
        
        # æ ¹æ®ç­–ç•¥è·å–promptæ¨¡æ¿
        if strategy and self.config_manager:
            prompt_template = self.config_manager.get_prompt_template(strategy)
        else:
            # ä½¿ç”¨é»˜è®¤æç®€æ¨¡æ¿
            prompt_template = """è¯·è¯„ä¼°ä»¥ä¸‹ç”¨æˆ·ä¸æ„å›¾çš„åŒ¹é…ç¨‹åº¦ï¼š

ã€æ„å›¾éœ€æ±‚ã€‘
{intent_description}{conditions}

ã€ç”¨æˆ·ä¿¡æ¯ã€‘
{profile_info}

è¯·ç»™å‡º0-1ä¹‹é—´çš„åŒ¹é…åˆ†æ•°ï¼Œå¹¶æä¾›ç®€çŸ­çš„ç†ç”±ã€‚

è¯„åˆ†æŒ‡å¯¼ï¼š
- é«˜åº¦åŒ¹é…ï¼ˆ0.7-1.0ï¼‰ï¼šæ ¸å¿ƒéœ€æ±‚åŸºæœ¬æ»¡è¶³
- ä¸­åº¦åŒ¹é…ï¼ˆ0.4-0.7ï¼‰ï¼šéƒ¨åˆ†ç¬¦åˆæˆ–æœ‰æ½œåœ¨ä»·å€¼  
- ä½åº¦åŒ¹é…ï¼ˆ0-0.4ï¼‰ï¼šç›¸å…³æ€§è¾ƒå¼±

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
    "match_score": 0.75,
    "confidence": 0.8,
    "is_match": true,
    "matched_aspects": ["ç¬¦åˆçš„æ–¹é¢"],
    "missing_aspects": ["ä¸ç¬¦åˆçš„æ–¹é¢"],
    "explanation": "ç®€çŸ­è§£é‡Š",
    "suggestions": "å»ºè®®"
}}"""
        
        # æ ¼å¼åŒ–prompt
        prompt = prompt_template.format(
            intent_description=intent_desc,
            conditions=conditions_text,
            profile_info=profile_text
        )
        
        return prompt
    
    def _build_mismatch_prompt(self, intent: Dict, profile: Dict) -> str:
        """æ„å»ºä¸åŒ¹é…è§£é‡Šçš„prompt"""
        return f"""åˆ†æä»¥ä¸‹æ„å›¾å’Œè”ç³»äººä¸ºä»€ä¹ˆä¸åŒ¹é…ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

æ„å›¾ï¼š{json.dumps(intent, ensure_ascii=False, indent=2)}

è”ç³»äººï¼š{json.dumps(profile, ensure_ascii=False, indent=2)}

è¯·æä¾›ï¼š
1. ä¸»è¦ä¸åŒ¹é…åŸå› ï¼ˆæœ€é‡è¦çš„3ä¸ªï¼‰
2. å¦‚æœè¦åŒ¹é…ï¼Œè”ç³»äººéœ€è¦å…·å¤‡ä»€ä¹ˆ
3. å¦‚æœè¦åŒ¹é…ï¼Œæ„å›¾å¯ä»¥å¦‚ä½•è°ƒæ•´
4. å…¶ä»–å¯èƒ½æ›´åˆé€‚çš„åŒ¹é…æ–¹å‘

è¦æ±‚å›ç­”ç®€æ´ã€å®ç”¨ã€æœ‰å»ºè®¾æ€§ã€‚"""
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLM API (ä½¿ç”¨å¼‚æ­¥æ–¹å¼çš„åŒæ­¥è¯·æ±‚)"""
        
        # æ„é€ è¯·æ±‚æ•°æ®
        data = {
            "model": "qwen-plus",  # æˆ– "qwen-max"
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ¹é…åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0,  # ä¿è¯ä¸€è‡´æ€§
            "max_tokens": 1000
        }
        
        try:
            # ä½¿ç”¨ asyncio è¿è¡ŒåŒæ­¥è¯·æ±‚
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: requests.post(
                    f"{self.api_endpoint}/chat/completions",
                    headers=self.headers,
                    data=json.dumps(data),
                    timeout=self.timeout
                )
            )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                logger.info(f"LLMå“åº”æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                return content
            else:
                error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                logger.error(error_msg)
                raise Exception(error_msg)
                
        except requests.Timeout:
            raise Exception(f"LLMè°ƒç”¨è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰")
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _parse_judgment(self, response: str) -> MatchJudgment:
        """è§£æLLMå“åº” - æç®€ç‰ˆæœ¬"""
        try:
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
            else:
                raise ValueError("å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°JSON")
            
            # è·å–åˆ†æ•°
            raw_score = float(data.get('match_score', 0))
            
            # åŸºæœ¬èŒƒå›´éªŒè¯ï¼ˆç¡®ä¿åœ¨0-1ä¹‹é—´ï¼‰
            validated_score = max(0.0, min(1.0, raw_score))
            
            # å¦‚æœåˆ†æ•°è¢«è°ƒæ•´ï¼Œè®°å½•æ—¥å¿—
            if validated_score != raw_score:
                logger.warning(f"âš ï¸ LLMåˆ†æ•°è¶…å‡ºèŒƒå›´ - åŸå§‹åˆ†æ•°:{raw_score:.3f}, è°ƒæ•´ä¸º:{validated_score:.3f}")
            else:
                logger.info(f"âœ… LLMè¯„åˆ†æœ‰æ•ˆ - åˆ†æ•°:{validated_score:.3f}")
            
            return MatchJudgment(
                match_score=validated_score,
                confidence=float(data.get('confidence', 0.8)),  # é»˜è®¤ç½®ä¿¡åº¦0.8
                is_match=bool(data.get('is_match', False)) or validated_score >= 0.50,  # 0.5åŠä»¥ä¸Šä¸ºåŒ¹é…
                matched_aspects=data.get('matched_aspects', []),
                missing_aspects=data.get('missing_aspects', []),
                explanation=data.get('explanation', ''),
                suggestions=data.get('suggestions'),
                strategy_used='llm'
            )
            
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}\nåŸå§‹å“åº”: {response}")
            # è¿”å›é»˜è®¤ç»“æœ
            return MatchJudgment(
                match_score=0.55,  # é»˜è®¤ç»™ä¸­ç­‰åˆ†æ•°
                confidence=0.5,
                is_match=True,  # é»˜è®¤ä¸ºæ½œåœ¨åŒ¹é…
                matched_aspects=[],
                missing_aspects=[],
                explanation=f"è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: {response[:200]}...",
                strategy_used='llm'
            )
    
    def _get_current_strategy(self) -> str:
        """
        è·å–å½“å‰åº”è¯¥ä½¿ç”¨çš„ç­–ç•¥ï¼ˆæ”¯æŒA/Bæµ‹è¯•ï¼‰
        
        Returns:
            ç­–ç•¥åç§°
        """
        if self.config_manager:
            return self.config_manager.get_ab_test_strategy()
        return "minimal"
    
    async def _apply_calibration(
        self,
        judgment: MatchJudgment,
        intent: Dict,
        profile: Dict
    ) -> MatchJudgment:
        """
        åº”ç”¨è‡ªé€‚åº”æ ¡å‡†
        
        Args:
            judgment: åŸå§‹åˆ¤æ–­ç»“æœ
            intent: æ„å›¾ä¿¡æ¯
            profile: è”ç³»äººä¿¡æ¯
            
        Returns:
            æ ¡å‡†åçš„åˆ¤æ–­ç»“æœ
        """
        if not self.analytics or not self.user_id:
            return judgment
        
        try:
            # è·å–æˆ–æ›´æ–°æ ¡å‡†å‚æ•°
            now = datetime.now()
            if (not self.calibration_params or 
                not self.calibration_update_time or
                now - self.calibration_update_time > timedelta(hours=1)):
                
                # è·å–æœ€æ–°æ ¡å‡†å‚æ•°
                self.calibration_params = self.analytics.calculate_calibration_params(
                    self.user_id,
                    min_feedback_count=self.config.calibration.get('min_feedback_count', 10)
                )
                self.calibration_update_time = now
                logger.info(f"ğŸ”§ æ›´æ–°æ ¡å‡†å‚æ•°: {self.calibration_params}")
            
            # åº”ç”¨æ ¡å‡†
            calibrated_score = judgment.match_score
            
            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´
            if judgment.confidence < self.calibration_params.get('confidence_threshold', 0.7):
                # ä½ç½®ä¿¡åº¦ï¼Œå‘ä¸­é—´å€¼é æ‹¢
                calibrated_score = calibrated_score * 0.8 + 0.5 * 0.2
            
            # æ ¹æ®åˆ†ç¦»åº¦è°ƒæ•´
            separation_factor = self.calibration_params.get('separation_factor', 1.0)
            if separation_factor != 1.0:
                # å¢å¼ºæ­£è´Ÿåé¦ˆçš„åŒºåˆ†åº¦
                if calibrated_score > 0.5:
                    calibrated_score = 0.5 + (calibrated_score - 0.5) * separation_factor
                else:
                    calibrated_score = 0.5 - (0.5 - calibrated_score) * separation_factor
            
            # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            calibrated_score = max(0.0, min(1.0, calibrated_score))
            
            # æ›´æ–°åˆ¤æ–­ç»“æœ
            judgment.match_score = calibrated_score
            judgment.is_match = calibrated_score >= self.config.thresholds.get('match_threshold', 0.5)
            
            return judgment
            
        except Exception as e:
            logger.warning(f"åº”ç”¨æ ¡å‡†å¤±è´¥: {e}")
            return judgment
    
    def update_feedback(
        self,
        intent_id: int,
        profile_id: int,
        feedback: str
    ) -> bool:
        """
        æ›´æ–°ç”¨æˆ·åé¦ˆï¼ˆç”¨äºæ•°æ®é£è½®ï¼‰
        
        Args:
            intent_id: æ„å›¾ID
            profile_id: è”ç³»äººID
            feedback: åé¦ˆç±»å‹ (positive/negative/neutral/ignored)
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if self.analytics and self.user_id:
            return self.analytics.update_feedback(
                self.user_id, intent_id, profile_id, feedback
            )
        return False
    
    def get_performance_stats(self, days: int = 7) -> Dict:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§å’Œä¼˜åŒ–ï¼‰
        
        Args:
            days: ç»Ÿè®¡å¤©æ•°
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡æ•°æ®
        """
        if self.analytics:
            return {
                'score_distribution': self.analytics.get_score_distribution(
                    user_id=self.user_id,
                    days=days
                ),
                'intent_performance': self.analytics.get_intent_type_performance(days),
                'quality_metrics': self.analytics.get_quality_metrics(days)
            }
        return {}
    
    def _validate_score_range(self, match_level: str, score: float) -> float:
        """
        éªŒè¯åˆ†æ•°æ˜¯å¦åœ¨å¯¹åº”çº§åˆ«èŒƒå›´å†…ï¼Œå¦‚æœä¸åœ¨åˆ™è°ƒæ•´åˆ°èŒƒå›´å†…
        
        Args:
            match_level: åŒ¹é…çº§åˆ« (A/B/C/D/E/F)
            score: åŸå§‹åˆ†æ•°
            
        Returns:
            è°ƒæ•´åçš„åˆ†æ•°
        """
        # å®šä¹‰å„çº§åˆ«çš„åˆ†æ•°èŒƒå›´
        level_ranges = {
            'A': (0.85, 1.00),
            'B': (0.70, 0.84),
            'C': (0.60, 0.69),
            'D': (0.50, 0.59),
            'E': (0.40, 0.49),
            'F': (0.00, 0.39)
        }
        
        # å¦‚æœæ²¡æœ‰çº§åˆ«ä¿¡æ¯ï¼Œæ ¹æ®åˆ†æ•°æ¨æ–­çº§åˆ«
        if not match_level or match_level not in level_ranges:
            if score >= 0.85:
                match_level = 'A'
            elif score >= 0.70:
                match_level = 'B'
            elif score >= 0.60:
                match_level = 'C'
            elif score >= 0.50:
                match_level = 'D'
            elif score >= 0.40:
                match_level = 'E'
            else:
                match_level = 'F'
            logger.info(f"æœªæä¾›çº§åˆ«ï¼Œæ ¹æ®åˆ†æ•°{score:.3f}æ¨æ–­ä¸º{match_level}çº§")
        
        # è·å–çº§åˆ«èŒƒå›´
        min_score, max_score = level_ranges[match_level]
        
        # éªŒè¯å’Œè°ƒæ•´åˆ†æ•°
        if score < min_score:
            # åˆ†æ•°åä½ï¼Œè°ƒæ•´åˆ°èŒƒå›´ä¸‹é™
            return min_score + 0.02  # ç¨é«˜äºä¸‹é™
        elif score > max_score:
            # åˆ†æ•°åé«˜ï¼Œè°ƒæ•´åˆ°èŒƒå›´ä¸Šé™
            return max_score - 0.01  # ç¨ä½äºä¸Šé™
        else:
            # åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            return score
    
    def _generate_cache_key(self, intent: Dict, profile: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨å…³é”®å­—æ®µç”Ÿæˆå“ˆå¸Œ
        key_data = {
            'intent_id': intent.get('id'),
            'intent_desc': intent.get('description', ''),
            'intent_conditions': intent.get('conditions', {}),
            'profile_id': profile.get('id'),
            'profile_name': profile.get('profile_name', ''),
            'profile_tags': profile.get('tags', []),
            'profile_basic': profile.get('basic_info', {})
        }
        
        key_str = json.dumps(key_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[MatchJudgment]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if cache_key in self.cache:
            cached_item = self.cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - cached_item['time'] < self.cache_ttl:
                return cached_item['judgment']
            else:
                # è¿‡æœŸåˆ™åˆ é™¤
                del self.cache[cache_key]
        return None
    
    def _save_to_cache(self, cache_key: str, judgment: MatchJudgment):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        self.cache[cache_key] = {
            'judgment': judgment,
            'time': datetime.now()
        }
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.cache) > 1000:
            # åˆ é™¤æœ€æ—§çš„é¡¹
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['time'])
            del self.cache[oldest_key]
    
    async def _filter_candidates(
        self,
        intent: Dict,
        profiles: List[Dict],
        strategy: MatchStrategy
    ) -> List[Dict]:
        """æ ¹æ®ç­–ç•¥è¿‡æ»¤å€™é€‰è”ç³»äºº"""
        
        if strategy == MatchStrategy.LLM_ONLY:
            # LLMå¤„ç†æ‰€æœ‰
            return profiles
            
        elif strategy == MatchStrategy.VECTOR_ONLY:
            # ä¸ä½¿ç”¨LLM
            return []
            
        elif strategy == MatchStrategy.HYBRID:
            # ä½¿ç”¨å‘é‡è¿‡æ»¤Top K
            try:
                from .vector_service import vector_service
                # è·å–å‘é‡ç›¸ä¼¼åº¦æ’åº
                scored_profiles = []
                for profile in profiles:
                    score, _ = await vector_service.calculate_semantic_similarity(
                        intent, profile, use_cache=True
                    )
                    scored_profiles.append((score, profile))
                
                # æ’åºå¹¶å–Top K
                scored_profiles.sort(key=lambda x: x[0], reverse=True)
                top_k = min(20, len(scored_profiles))
                return [p for _, p in scored_profiles[:top_k]]
                
            except Exception as e:
                logger.error(f"å‘é‡è¿‡æ»¤å¤±è´¥: {e}")
                return profiles[:20]  # é™çº§å¤„ç†
                
        else:  # ADAPTIVE
            # æ ¹æ®å¤æ‚åº¦è‡ªé€‚åº”
            complexity = self.assess_complexity(intent)
            
            if complexity == ComplexityLevel.SIMPLE:
                # ç®€å•æ„å›¾ä¸ä½¿ç”¨LLM
                return []
            elif complexity == ComplexityLevel.MODERATE:
                # ä¸­ç­‰å¤æ‚åº¦ï¼Œé€‰æ‹©å‰10ä¸ª
                return profiles[:10]
            else:
                # å¤æ‚æ„å›¾ï¼Œé€‰æ‹©å‰20ä¸ª
                return profiles[:20]
    
    async def _parallel_judge(
        self,
        intent: Dict,
        profiles: List[Dict]
    ) -> List[MatchJudgment]:
        """å¹¶è¡Œåˆ¤æ–­å¤šä¸ªè”ç³»äºº"""
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for profile in profiles:
            task = self.judge_match(intent, profile)
            tasks.append(task)
        
        # åˆ†æ‰¹æ‰§è¡Œ
        results = []
        for i in range(0, len(tasks), self.max_parallel):
            batch = tasks[i:i + self.max_parallel]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"å¹¶è¡Œåˆ¤æ–­å¤±è´¥: {result}")
                    # æ·»åŠ å¤±è´¥ç»“æœ
                    results.append(MatchJudgment(
                        match_score=0.0,
                        confidence=0.0,
                        is_match=False,
                        matched_aspects=[],
                        missing_aspects=[],
                        explanation=f"åˆ¤æ–­å¤±è´¥: {str(result)}"
                    ))
                else:
                    results.append(result)
        
        return results

# å…¨å±€å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
llm_matching_service = None

def init_llm_matching_service(api_key: str, db_path: str = "user_profiles.db", api_endpoint: str = None, user_id: str = None):
    """åˆå§‹åŒ–å…¨å±€LLMåŒ¹é…æœåŠ¡ï¼ˆæ”¯æŒæ•°æ®é£è½®ï¼‰"""
    global llm_matching_service
    llm_matching_service = LLMMatchingService(api_key, db_path, api_endpoint, user_id)
    return llm_matching_service