# generate_comparison_report.py
"""
ç”ŸæˆPromptå¯¹æ¯”æµ‹è¯•çš„MarkdownæŠ¥å‘Š
"""
import json
import sys
from datetime import datetime


def generate_markdown_report(results_file: str):
    """ä»JSONç»“æœç”ŸæˆMarkdownæŠ¥å‘Š"""

    # è¯»å–æµ‹è¯•ç»“æœ
    with open(results_file, 'r', encoding='utf-8') as f:
        results = json.load(f)

    metadata = results['metadata']
    stats = results['statistics']
    test_cases = results['test_cases']

    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
    report_file = results_file.replace('.json', '_report.md')

    with open(report_file, 'w', encoding='utf-8') as f:
        # æ ‡é¢˜
        f.write(f"# Prompt v2.0 å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {metadata['test_time']}\n\n")
        f.write(f"**æµ‹è¯•ç”¨ä¾‹**: {metadata['total_cases']}ä¸ª\n\n")

        # æ•´ä½“è¡¨ç°
        f.write("## ğŸ“Š æ•´ä½“è¡¨ç°å¯¹æ¯”\n\n")

        f.write("| æŒ‡æ ‡ | v1.0 | v2.0 | æ”¹è¿› |\n")
        f.write("|------|------|------|------|\n")

        # æ–‡å­—å‡†ç¡®ç‡
        v1_text = stats['v1']['text_correct_rate']
        v2_text = stats['v2']['text_correct_rate']
        text_diff = v2_text - v1_text
        text_arrow = "â†‘" if text_diff > 0 else "â†“" if text_diff < 0 else "â†’"
        f.write(f"| æ–‡å­—åˆ¤æ–­å‡†ç¡®ç‡ | {v1_text:.1f}% | {v2_text:.1f}% | {text_diff:+.1f}% {text_arrow} |\n")

        # å¹³å‡é•¿åº¦
        v1_len = stats['v1']['avg_length']
        v2_len = stats['v2']['avg_length']
        len_diff = v2_len - v1_len
        len_percent = (len_diff / v1_len * 100) if v1_len > 0 else 0
        len_arrow = "â†“" if len_diff < 0 else "â†‘" if len_diff > 0 else "â†’"
        f.write(f"| å¹³å‡Prompté•¿åº¦ | {v1_len:.0f}å­— | {v2_len:.0f}å­— | {len_percent:+.1f}% {len_arrow} |\n")

        # å¹³å‡é—®é¢˜æ•°
        v1_issues = stats['v1']['avg_issues']
        v2_issues = stats['v2']['avg_issues']
        issues_diff = v2_issues - v1_issues
        issues_arrow = "â†“" if issues_diff < 0 else "â†‘" if issues_diff > 0 else "â†’"
        f.write(f"| å¹³å‡è´¨é‡é—®é¢˜æ•° | {v1_issues:.2f} | {v2_issues:.2f} | {issues_diff:+.2f} {issues_arrow} |\n")

        f.write("\n")

        # åˆ†ç±»è¡¨ç°
        f.write("## ğŸ“ˆ åˆ†ç±»è¡¨ç°åˆ†æ\n\n")

        for category, data in stats['categories'].items():
            total = data['total']
            v1_correct = data['v1_correct']
            v2_correct = data['v2_correct']
            v1_rate = v1_correct / total * 100
            v2_rate = v2_correct / total * 100
            improvement = v2_rate - v1_rate

            f.write(f"### {category}\n\n")
            f.write(f"- **v1.0**: {v1_rate:.0f}% ({v1_correct}/{total})\n")
            f.write(f"- **v2.0**: {v2_rate:.0f}% ({v2_correct}/{total})\n")

            if improvement > 0:
                f.write(f"- **æ”¹è¿›**: +{improvement:.0f}% âœ… æ˜¾è‘—æå‡\n")
            elif improvement < 0:
                f.write(f"- **å˜åŒ–**: {improvement:.0f}% âš ï¸ éœ€è¦å…³æ³¨\n")
            else:
                f.write(f"- **ä¿æŒ**: æŒå¹³\n")

            f.write("\n")

        # ä¼˜ç§€æ¡ˆä¾‹
        f.write("## âœ¨ ä¼˜ç§€æ¡ˆä¾‹å±•ç¤º\n\n")

        # æ‰¾å‡ºv2.0è¡¨ç°ä¼˜ç§€çš„æ¡ˆä¾‹
        excellent_cases = [
            case for case in test_cases
            if case['v2'].get('analysis', {}).get('text_correct') == True
            and len(case['v2'].get('analysis', {}).get('issues', [])) == 0
        ]

        for case in excellent_cases[:3]:  # å±•ç¤ºå‰3ä¸ª
            emotion = case['emotion']
            category = case['category']
            v2_prompt = case['v2']['prompt']

            f.write(f"### {emotion}ï¼ˆ{category}ï¼‰\n\n")
            f.write(f"```\n{v2_prompt}\n```\n\n")

            # åˆ†æäº®ç‚¹
            f.write("**äº®ç‚¹**:\n")
            if case['v2'].get('analysis', {}).get('has_text'):
                f.write("- âœ… æ­£ç¡®æ·»åŠ æ–‡å­—\n")
            else:
                f.write("- âœ… æ­£ç¡®ä¸æ·»åŠ æ–‡å­—\n")
            f.write("- âœ… åŒ…å«å¯çˆ±åŠ¨ç‰©è§’è‰²\n")
            f.write("- âœ… è¡¨æƒ…åŠ¨ä½œç”ŸåŠ¨\n")
            f.write("- âœ… é£æ ¼æ ‡ç­¾å®Œæ•´\n")
            f.write("\n")

        # éœ€è¦æ”¹è¿›çš„æ¡ˆä¾‹
        f.write("## âš ï¸ éœ€è¦æ”¹è¿›çš„æ¡ˆä¾‹\n\n")

        problem_cases = [
            case for case in test_cases
            if case['v2'].get('analysis', {}).get('text_correct') == False
            or len(case['v2'].get('analysis', {}).get('issues', [])) > 0
        ]

        if problem_cases:
            for case in problem_cases[:3]:  # å±•ç¤ºå‰3ä¸ª
                emotion = case['emotion']
                v2_issues = case['v2'].get('analysis', {}).get('issues', [])

                f.write(f"### {emotion}\n\n")
                f.write(f"**é—®é¢˜**:\n")
                for issue in v2_issues:
                    f.write(f"- âŒ {issue}\n")

                # å¦‚æœæ–‡å­—åˆ¤æ–­é”™è¯¯
                if case['v2'].get('analysis', {}).get('text_correct') == False:
                    expected = case['expected_text']
                    actual = case['v2'].get('analysis', {}).get('has_text')
                    if expected and not actual:
                        f.write(f"- âŒ åº”è¯¥æ·»åŠ æ–‡å­—ä½†æœªæ·»åŠ \n")
                    elif not expected and actual:
                        f.write(f"- âŒ ä¸åº”è¯¥æ·»åŠ æ–‡å­—ä½†æ·»åŠ äº†\n")

                f.write("\n")
        else:
            f.write("ğŸ‰ æ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜ï¼\n\n")

        # å¯¹æ¯”è¯¦æƒ…è¡¨
        f.write("## ğŸ“‹ è¯¦ç»†å¯¹æ¯”è¡¨\n\n")

        f.write("| æƒ…ç»ª | åˆ†ç±» | v1æ–‡å­— | v2æ–‡å­— | åˆ¤æ–­ | v1é•¿åº¦ | v2é•¿åº¦ | v2é—®é¢˜æ•° |\n")
        f.write("|------|------|--------|--------|------|--------|--------|----------|\n")

        for case in test_cases:
            emotion = case['emotion']
            category = case['category']

            v1_text = "âœ…" if case['v1'].get('analysis', {}).get('has_text') else "âŒ"
            v2_text = "âœ…" if case['v2'].get('analysis', {}).get('has_text') else "âŒ"

            v2_correct = case['v2'].get('analysis', {}).get('text_correct')
            correct_icon = "âœ…" if v2_correct == True else "âŒ" if v2_correct == False else "?"

            v1_len = case['v1'].get('analysis', {}).get('length', 0)
            v2_len = case['v2'].get('analysis', {}).get('length', 0)

            v2_issues = len(case['v2'].get('analysis', {}).get('issues', []))

            f.write(f"| {emotion} | {category} | {v1_text} | {v2_text} | {correct_icon} | {v1_len} | {v2_len} | {v2_issues} |\n")

        f.write("\n")

        # æ€»ç»“å»ºè®®
        f.write("## ğŸ’¡ æ€»ç»“ä¸å»ºè®®\n\n")

        # æ ¹æ®æ”¹è¿›æƒ…å†µç»™å‡ºå»ºè®®
        text_improvement = stats['improvement']['text_accuracy']

        f.write("### æ”¹è¿›æ•ˆæœ\n\n")

        if text_improvement >= 10:
            f.write(f"- âœ… æ–‡å­—åˆ¤æ–­å‡†ç¡®ç‡æå‡{text_improvement:.1f}%ï¼Œæ•ˆæœæ˜¾è‘—\n")
        elif text_improvement >= 5:
            f.write(f"- âœ… æ–‡å­—åˆ¤æ–­å‡†ç¡®ç‡æå‡{text_improvement:.1f}%ï¼Œæ•ˆæœè‰¯å¥½\n")
        elif text_improvement >= 0:
            f.write(f"- âš–ï¸ æ–‡å­—åˆ¤æ–­å‡†ç¡®ç‡æå‡{text_improvement:.1f}%ï¼Œç•¥æœ‰æ”¹è¿›\n")
        else:
            f.write(f"- âš ï¸ æ–‡å­—åˆ¤æ–­å‡†ç¡®ç‡ä¸‹é™{abs(text_improvement):.1f}%ï¼Œéœ€è¦ä¼˜åŒ–\n")

        if len_percent < -20:
            f.write(f"- âœ… Prompté•¿åº¦å‡å°‘{abs(len_percent):.1f}%ï¼ŒTokenæ•ˆç‡æ˜¾è‘—æå‡\n")
        elif len_percent < -10:
            f.write(f"- âœ… Prompté•¿åº¦å‡å°‘{abs(len_percent):.1f}%ï¼ŒTokenæ•ˆç‡æå‡\n")

        quality_improvement = stats['improvement']['quality_issues']
        if quality_improvement < -0.3:
            f.write(f"- âœ… è´¨é‡é—®é¢˜å‡å°‘{abs(quality_improvement):.2f}ä¸ªï¼Œç”Ÿæˆè´¨é‡æå‡\n")

        f.write("\n### ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n")

        # æ ¹æ®é—®é¢˜æ¡ˆä¾‹æ•°é‡ç»™å»ºè®®
        if len(problem_cases) == 0:
            f.write("- âœ… æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼Œå¯ä»¥æ­£å¼ä¸Šçº¿v2.0\n")
            f.write("- ğŸ“Š å»ºè®®æ”¶é›†çœŸå®ç”¨æˆ·åé¦ˆï¼ŒæŒç»­ä¼˜åŒ–\n")
        elif len(problem_cases) <= 3:
            f.write("- âš ï¸ å°‘æ•°æ¡ˆä¾‹éœ€è¦ä¼˜åŒ–ï¼Œå»ºè®®é’ˆå¯¹æ€§è°ƒæ•´Prompt\n")
            f.write("- ğŸ”§ ä¼˜åŒ–åé‡æ–°æµ‹è¯•é—®é¢˜æ¡ˆä¾‹\n")
        else:
            f.write("- âš ï¸ è¾ƒå¤šæ¡ˆä¾‹å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®å…¨é¢review Promptè®¾è®¡\n")
            f.write("- ğŸ” é‡ç‚¹æ£€æŸ¥æ–‡å­—åˆ¤æ–­é€»è¾‘å’Œç¤ºä¾‹è¦†ç›–\n")

        f.write("- ğŸ¨ å¯é€‰ï¼šç”Ÿæˆå®é™…å›¾ç‰‡è¿›è¡Œäººå·¥è§†è§‰è¯„ä¼°\n")
        f.write("- ğŸ“ˆ å»ºè®®è®¾ç½®ç›‘æ§æŒ‡æ ‡ï¼ŒæŒç»­è·Ÿè¸ªç”Ÿæˆè´¨é‡\n")

        f.write("\n---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    return report_file


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_comparison_report.py <results.json>")
        print("ç¤ºä¾‹: python generate_comparison_report.py tests/results/prompt_comparison_20251002_143000.json")
        sys.exit(1)

    results_file = sys.argv[1]

    if not results_file.endswith('.json'):
        print("âŒ é”™è¯¯: éœ€è¦JSONæ–‡ä»¶")
        sys.exit(1)

    try:
        report_file = generate_markdown_report(results_file)
        print(f"\nğŸ“– æŸ¥çœ‹æŠ¥å‘Š: cat {report_file}")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {results_file}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
