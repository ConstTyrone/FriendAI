# Prompt v2.0 测试执行指南

## 📋 测试目标

通过对比测试验证Prompt v2.0相对于v1.0的改进效果：
1. **文字判断准确率**：是否正确判断该添加/不添加文字
2. **描述质量**：生成的prompt是否包含必要元素
3. **场景覆盖**：新增场景（YYDS、打工人等）是否正确处理
4. **Token效率**：生成的prompt长度是否合理

## 🚀 快速开始

### 步骤1：运行自动化测试

```bash
# 进入项目目录
cd /Users/wzy668/Projects/FriendAI/wechat-customer-service

# 运行测试脚本
python tests/test_prompt_comparison.py --confirm
```

**预计耗时**：约30分钟（15个用例 × 2个版本 × 1分钟/用例）

**输出结果**：
- 控制台：实时进度和统计报告
- 文件：`tests/results/prompt_comparison_YYYYMMDD_HHMMSS.json`

### 步骤2：查看自动化测试报告

测试完成后会输出：

```
📈 统计报告
==================================================
总测试用例: 15

【v1.0 版本】
  生成成功率: 100.0%
  文字判断准确率: 73.3%
  平均描述长度: 245字
  平均问题数: 0.8

【v2.0 版本】
  生成成功率: 100.0%
  文字判断准确率: 93.3%
  平均描述长度: 168字
  平均问题数: 0.2

【改进情况】
  文字准确率提升: +20.0%
  质量问题减少: -0.6个
```

## 📊 测试用例说明

### 15个测试场景

| # | 情绪/场景 | 分类 | 预期文字 | 测试目的 |
|---|----------|------|---------|---------|
| 1 | 开心 | 基础情绪 | ✅ 需要 | 强烈情绪判断 |
| 2 | 累了 | 基础情绪 | ✅ 需要 | 疲惫状态表达 |
| 3 | 生气 | 基础情绪 | ✅ 需要 | 负面情绪处理 |
| 4 | YYDS | 网络流行语 | ✅ 需要 | 新场景-流行语 |
| 5 | 破防了 | 网络流行语 | ✅ 需要 | 新场景-情绪梗 |
| 6 | 绝绝子 | 网络流行语 | ✅ 需要 | 新场景-夸张梗 |
| 7 | 打工人 | 社交场景 | ✅ 需要 | 新场景-职场 |
| 8 | 摸鱼 | 社交场景 | ❌ 不需要 | 新场景-动作暗示 |
| 9 | 下班 | 社交场景 | ✅ 需要 | 新场景-状态梗 |
| 10 | 微笑 | 社交礼仪 | ❌ 不需要 | 无文字-礼貌 |
| 11 | 点头 | 社交礼仪 | ❌ 不需要 | 无文字-确认 |
| 12 | OK | 社交礼仪 | ❌ 不需要 | 无文字-手势 |
| 13 | 得意 | 复合情绪 | ✅ 需要 | 新场景-得意情绪 |
| 14 | 惊讶 | 复合情绪 | ✅ 需要 | 新场景-惊讶反应 |
| 15 | 害羞 | 复合情绪 | ❌ 不需要 | 新场景-羞涩表情 |

### 质量评估指标

每个生成的prompt会被自动分析：

1. **文字判断** ✅/❌
   - 是否正确添加/不添加文字
   - 与预期是否一致

2. **必要元素检查**：
   - ✅ 包含动物角色（柴犬、小猫等）
   - ✅ 包含表情描述（眼睛、嘴巴、笑容）
   - ✅ 包含风格标签（Q版卡通、可爱萌系）

3. **长度合理性**：
   - ⚠️ <50字：描述过短
   - ✅ 50-400字：合理范围
   - ⚠️ >400字：描述过长

4. **问题检测**：
   - 缺少动物角色
   - 缺少表情描述
   - 缺少风格标签
   - 长度不合理

## 🎨 人工质量评分（可选）

### 为什么需要人工评分？

自动化测试只能检测**结构性指标**，无法判断：
- 描述是否生动有趣
- 情绪表达是否到位
- 整体美感和创意

### 人工评分流程

#### 步骤1：准备评分表格

创建 `tests/results/manual_scoring.xlsx`：

| 情绪 | v1.0评分 | v2.0评分 | 备注 |
|------|---------|---------|------|
| 开心 | ? | ? | |
| 累了 | ? | ? | |
| ... | ? | ? | |

**评分标准（1-5分）**：
- 5分：非常好，完美表达情绪，生动有趣
- 4分：好，正确表达，略有小瑕疵
- 3分：一般，基本表达，但不够出彩
- 2分：差，表达不准确或缺少重要元素
- 1分：很差，完全不符合要求

#### 步骤2：实际生成图片（可选）

如果想看到最终效果：

```python
# 使用测试结果中的prompt生成图片
from src.services.image_service import image_service

# 从JSON文件读取测试结果
import json
with open('tests/results/prompt_comparison_20251002_143000.json') as f:
    results = json.load(f)

# 选择一个测试用例
test_case = results['test_cases'][0]  # 第一个：开心
v2_prompt = test_case['v2']['prompt']

# 生成图片
result = image_service.generate_image(prompt=v2_prompt)
print(f"图片已生成: {result['image_path']}")
```

#### 步骤3：填写评分表

对比v1.0和v2.0生成的prompt（或实际图片），填写评分和备注。

#### 步骤4：计算平均分

```
v1.0平均分 = (所有v1.0评分总和) / 15
v2.0平均分 = (所有v2.0评分总和) / 15
改进幅度 = v2.0平均分 - v1.0平均分
```

## 📈 生成对比报告

### 自动生成markdown报告

```bash
python tests/generate_comparison_report.py tests/results/prompt_comparison_20251002_143000.json
```

会生成 `tests/results/comparison_report.md`，包含：
- 统计数据可视化
- 分类表现对比
- 具体案例展示
- 改进建议

### 报告内容示例

```markdown
# Prompt v2.0 对比测试报告

## 整体表现

| 指标 | v1.0 | v2.0 | 改进 |
|------|------|------|------|
| 文字准确率 | 73.3% | 93.3% | +20.0% ↑ |
| 平均长度 | 245字 | 168字 | -31.4% ↓ |
| 平均问题数 | 0.8 | 0.2 | -75% ↓ |

## 分类表现

### 网络流行语 (新增场景)
- v1.0: 66.7% (2/3)  ← 对新梗理解不足
- v2.0: 100% (3/3)   ← 完美处理

### 优秀案例

**YYDS（v2.0）**:
```
橘色小老虎双手高举奖杯，眼睛发光，嘴巴大张欢呼，
尾巴炸毛兴奋状，正上方金色超粗体"YYDS"，
文字带彩虹渐变多层描边星光爆炸效果...
```
✅ 完美理解"永远的神"，文字处理正确
✅ 描述生动，情绪饱满
```

## 📝 收集用户反馈

### 内测用户测试

如果有实际用户，可以进行A/B测试：

#### 方案A：随机分配
```python
# 50%用户使用v1.0，50%使用v2.0
import random

def get_prompt_version(user_id):
    return 'v2' if hash(user_id) % 2 == 0 else 'v1'
```

#### 方案B：时间分段
```
第1周：全部使用v1.0，收集反馈
第2周：全部使用v2.0，收集反馈
对比：用户满意度、生成成功率、投诉数量
```

### 反馈收集表单

创建简单的问卷：

**问题1**：您对生成的表情包满意吗？
- ⭐⭐⭐⭐⭐ 非常满意
- ⭐⭐⭐⭐ 满意
- ⭐⭐⭐ 一般
- ⭐⭐ 不满意
- ⭐ 非常不满意

**问题2**：文字添加是否合理？
- 完全合理
- 基本合理
- 有时不合理
- 经常不合理

**问题3**：最喜欢/最不满意的是什么？（开放题）

### 数据埋点

```python
# 记录用户行为
def track_emoticon_feedback(user_id, emotion, version, satisfaction):
    """
    satisfaction: 1-5星评分
    """
    audit_db.record_feedback(
        user_id=user_id,
        emotion=emotion,
        prompt_version=version,
        satisfaction=satisfaction,
        timestamp=datetime.now()
    )
```

## 🎯 成功标准

### 自动化测试通过标准

- ✅ 文字判断准确率 ≥ 85%
- ✅ 生成成功率 = 100%
- ✅ 平均质量问题数 ≤ 0.5
- ✅ 新场景覆盖率 = 100%

### 人工评分通过标准

- ✅ v2.0平均分 ≥ 4.0
- ✅ v2.0平均分 > v1.0平均分
- ✅ 无严重质量问题（评分1-2分）

### 用户反馈通过标准

- ✅ 用户满意度 ≥ 80%
- ✅ 投诉率下降或持平
- ✅ 生成成功率提升或持平

## 🔧 问题修复流程

如果测试发现问题：

### 1. 分析失败案例
```bash
# 查看JSON结果文件
cat tests/results/prompt_comparison_20251002_143000.json | jq '.test_cases[] | select(.v2.analysis.text_correct == false)'
```

### 2. 定位问题原因
- 是Prompt规则不清晰？
- 是示例不够典型？
- 是AI理解偏差？

### 3. 调整Prompt
修改 `src/services/emoticon_service.py` 中的 `EMOTICON_SYSTEM_PROMPT`

### 4. 重新测试
```bash
python tests/test_prompt_comparison.py --confirm
```

### 5. 对比改进
对比修复前后的测试结果JSON文件

## 📚 附录：常见问题

### Q1: 测试太慢怎么办？
A: 可以先测试部分用例：
```python
# 修改 test_prompt_comparison.py
TEST_CASES = TEST_CASES[:5]  # 只测试前5个
```

### Q2: 如何只测试某个分类？
A:
```python
TEST_CASES = [case for case in TEST_CASES if case['category'] == '网络流行语']
```

### Q3: 如何查看具体的prompt对比？
A: 打开JSON结果文件，找到对应的test_case：
```json
{
  "emotion": "YYDS",
  "v1": {
    "prompt": "...",
    "analysis": {...}
  },
  "v2": {
    "prompt": "...",
    "analysis": {...}
  }
}
```

### Q4: 测试失败了怎么办？
A: 检查：
1. 环境变量是否配置正确（QWEN_API_KEY）
2. 网络是否正常
3. API配额是否用完

---

**下一步**：运行测试后，查看 `comparison_report.md` 了解详细结果！
